{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gpjvVlLnTS2"
      },
      "outputs": [],
      "source": [
        "# churn_prediction.py\n",
        "\"\"\"\n",
        "Telco Customer Churn Prediction (IBM Telco Customer Churn)\n",
        "- Loads dataset (auto-download from Kaggle if configured, or local CSV fallback)\n",
        "- Preprocesses (encoding, scaling)\n",
        "- Trains Logistic Regression and RandomForest\n",
        "- Evaluates models (accuracy, ROC AUC, confusion matrix, classification report)\n",
        "- Saves artifacts (models + preprocessing)\n",
        "- Interactive prediction mode (choose model and enter feature values)\n",
        "\n",
        "Dataset reference: Kaggle \"Telco Customer Churn\" (blastchar). See dataset page for CSV and details.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
        ")\n",
        "\n",
        "# ---------- User-configurable ----------\n",
        "KAGGLE_AUTO_DOWNLOAD = True   # If True, try to download using Kaggle CLI (requires kaggle.json)\n",
        "DATA_FILENAME = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "ARTIFACTS_DIR = \"churn_artifacts\"\n",
        "RANDOM_STATE = 42\n",
        "# --------------------------------------\n",
        "\n",
        "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
        "\n",
        "def download_from_kaggle():\n",
        "    \"\"\"\n",
        "    Attempt to download dataset using Kaggle API.\n",
        "    Requires 'kaggle' python package or kaggle CLI configured with ~/.kaggle/kaggle.json\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # prefer Kaggle python package if installed\n",
        "        import kaggle\n",
        "        print(\"Found kaggle python package; attempting download...\")\n",
        "        # dataset slug\n",
        "        dataset = \"blastchar/telco-customer-churn\"\n",
        "        kaggle.api.dataset_download_files(dataset, path=\".\", unzip=True, quiet=False)\n",
        "        print(\"Download complete.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        # try system kaggle CLI\n",
        "        try:\n",
        "            print(\"kaggle python package not available or failed. Trying system 'kaggle' CLI...\")\n",
        "            exit_code = os.system(f'kaggle datasets download -d blastchar/telco-customer-churn -p . --unzip')\n",
        "            if exit_code == 0:\n",
        "                print(\"Downloaded via kaggle CLI.\")\n",
        "                return True\n",
        "            else:\n",
        "                print(\"kaggle CLI download failed (exit code != 0).\")\n",
        "                return False\n",
        "        except Exception as ex:\n",
        "            print(\"Automatic download failed:\", ex)\n",
        "            return False\n",
        "\n",
        "def load_dataset():\n",
        "    # Try local first\n",
        "    if Path(DATA_FILENAME).exists():\n",
        "        print(f\"Loading dataset from local file: {DATA_FILENAME}\")\n",
        "        df = pd.read_csv(DATA_FILENAME)\n",
        "        return df\n",
        "    if KAGGLE_AUTO_DOWNLOAD:\n",
        "        print(\"Local dataset not found. Attempting Kaggle download (requires kaggle.json or CLI configured).\")\n",
        "        success = download_from_kaggle()\n",
        "        if success and Path(DATA_FILENAME).exists():\n",
        "            print(\"Loading dataset after download.\")\n",
        "            df = pd.read_csv(DATA_FILENAME)\n",
        "            return df\n",
        "        else:\n",
        "            print(\"Automatic download failed. Please download the dataset CSV from Kaggle and place it here:\")\n",
        "            print(\"https://www.kaggle.com/datasets/blastchar/telco-customer-churn\")\n",
        "            sys.exit(1)\n",
        "    else:\n",
        "        print(\"Dataset not found and auto-download disabled. Please place the CSV in the script folder.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "def preprocess(df, fit_transform=True, preprocessor=None):\n",
        "    \"\"\"\n",
        "    Preprocess the Telco dataset:\n",
        "    - Fix total charges numeric conversion\n",
        "    - Drop customerID\n",
        "    - Encode categorical features with OneHotEncoder (drop='first' is optional)\n",
        "    - Scale numeric features with StandardScaler\n",
        "    Returns (X_transformed, y, preprocessor)\n",
        "    If fit_transform=False, expects preprocessor to be a fitted ColumnTransformer.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    # Target\n",
        "    if 'Churn' not in df.columns:\n",
        "        raise ValueError(\"Expected 'Churn' column in dataset.\")\n",
        "    y = (df['Churn'].str.strip().map({'Yes':1, 'No':0})).astype(int)\n",
        "\n",
        "    # Drop ID if present\n",
        "    if 'customerID' in df.columns:\n",
        "        df = df.drop(columns=['customerID'])\n",
        "\n",
        "    # Clean TotalCharges (some rows are blank or spaces)\n",
        "    if 'TotalCharges' in df.columns:\n",
        "        df['TotalCharges'] = df['TotalCharges'].replace(\" \", np.nan)\n",
        "        df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "        # fill na with 0 or with MonthlyCharges * tenure as an option\n",
        "        df['TotalCharges'] = df['TotalCharges'].fillna(df['MonthlyCharges'] * df['tenure'])\n",
        "\n",
        "    # Separate features\n",
        "    X = df.drop(columns=['Churn'])\n",
        "\n",
        "    # Identify categorical vs numeric\n",
        "    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # For binary-like object columns with 'Yes'/'No' or 'Male'/'Female' we can map to 0/1 first\n",
        "    # But we'll let OneHotEncoder handle categoricals for simplicity\n",
        "    # Create ColumnTransformer\n",
        "    if fit_transform:\n",
        "        numeric_transformer = Pipeline(steps=[\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "        ])\n",
        "        from sklearn.compose import ColumnTransformer\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_transformer, numeric_cols),\n",
        "                ('cat', categorical_transformer, categorical_cols)\n",
        "            ],\n",
        "            remainder='drop'\n",
        "        )\n",
        "        X_transformed = preprocessor.fit_transform(X)\n",
        "        # Get feature names (for later interpretability)\n",
        "        # OneHotEncoder categories\n",
        "        ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
        "        ohe_cat_names = []\n",
        "        if hasattr(ohe, 'get_feature_names_out'):\n",
        "            ohe_cat_names = list(ohe.get_feature_names_out(categorical_cols))\n",
        "        else:\n",
        "            # sklearn < 1.0 fallback\n",
        "            for i, col in enumerate(categorical_cols):\n",
        "                cats = ohe.categories_[i]\n",
        "                ohe_cat_names += [f\"{col}_{c}\" for c in cats]\n",
        "        feature_names = list(numeric_cols) + ohe_cat_names\n",
        "        return X_transformed, y, preprocessor, feature_names\n",
        "    else:\n",
        "        X_transformed = preprocessor.transform(X)\n",
        "        # feature_names must be provided separately if needed\n",
        "        return X_transformed, y, preprocessor, None\n",
        "\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test, feature_names):\n",
        "    # Logistic Regression\n",
        "    log = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
        "    log.fit(X_train, y_train)\n",
        "    y_pred_log = log.predict(X_test)\n",
        "    y_prob_log = log.predict_proba(X_test)[:,1]\n",
        "    acc_log = accuracy_score(y_test, y_pred_log)\n",
        "    auc_log = roc_auc_score(y_test, y_prob_log)\n",
        "\n",
        "    print(\"\\n=== Logistic Regression Results ===\")\n",
        "    print(\"Accuracy: {:.4f}\".format(acc_log))\n",
        "    print(\"ROC AUC: {:.4f}\".format(auc_log))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred_log))\n",
        "\n",
        "    # Random Forest\n",
        "    rf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred_rf = rf.predict(X_test)\n",
        "    y_prob_rf = rf.predict_proba(X_test)[:,1]\n",
        "    acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "    auc_rf = roc_auc_score(y_test, y_prob_rf)\n",
        "\n",
        "    print(\"\\n=== Random Forest Results ===\")\n",
        "    print(\"Accuracy: {:.4f}\".format(acc_rf))\n",
        "    print(\"ROC AUC: {:.4f}\".format(auc_rf))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "\n",
        "    # Feature importances from RF (map back to feature names)\n",
        "    try:\n",
        "        importances = rf.feature_importances_\n",
        "        if feature_names is not None and len(feature_names) == len(importances):\n",
        "            fi = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)[:20]\n",
        "            print(\"\\nTop 20 feature importances (Random Forest):\")\n",
        "            for name, val in fi:\n",
        "                print(f\"  {name}: {val:.4f}\")\n",
        "        else:\n",
        "            print(\"\\nFeature importances computed but feature names unavailable or mismatch.\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not compute feature importances:\", e)\n",
        "\n",
        "    results = {\n",
        "        'logistic': {'accuracy': float(acc_log), 'roc_auc': float(auc_log)},\n",
        "        'random_forest': {'accuracy': float(acc_rf), 'roc_auc': float(auc_rf)}\n",
        "    }\n",
        "    return log, rf, results\n",
        "\n",
        "def save_artifacts(preprocessor, feature_names, log_model, rf_model, results):\n",
        "    with open(os.path.join(ARTIFACTS_DIR, 'preprocessor.pkl'), 'wb') as f:\n",
        "        pickle.dump(preprocessor, f)\n",
        "    with open(os.path.join(ARTIFACTS_DIR, 'feature_names.json'), 'w') as f:\n",
        "        json.dump(feature_names, f)\n",
        "    with open(os.path.join(ARTIFACTS_DIR, 'logistic_model.pkl'), 'wb') as f:\n",
        "        pickle.dump(log_model, f)\n",
        "    with open(os.path.join(ARTIFACTS_DIR, 'rf_model.pkl'), 'wb') as f:\n",
        "        pickle.dump(rf_model, f)\n",
        "    with open(os.path.join(ARTIFACTS_DIR, 'results_summary.json'), 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"Saved artifacts to '{ARTIFACTS_DIR}'\")\n",
        "\n",
        "def interactive_prediction(preprocessor, feature_names, log_model, rf_model):\n",
        "    \"\"\"\n",
        "    Prompt user for input values for a new customer.\n",
        "    We'll request values for the original input columns in a friendly order.\n",
        "    The preprocessor expects the original columns (numeric + categorical).\n",
        "    To simplify, load a sample row of the training data columns names from feature_names via the preprocessor.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Interactive Prediction Mode ---\")\n",
        "    print(\"You will be prompted to enter values for features. Press ENTER to use defaults shown in [] brackets.\")\n",
        "    # To get original column names, attempt to reconstruct from preprocessor\n",
        "    # preprocessor.transformers_ contains the mapping\n",
        "    col_names = []\n",
        "    # Try to recover original columns from transformers\n",
        "    try:\n",
        "        transformers = preprocessor.transformers_\n",
        "        # transformers is a list like [('num', Pipeline, [num_cols]), ('cat', Pipeline, [cat_cols])]\n",
        "        for name, trans, cols in transformers:\n",
        "            if cols == 'remainder' or cols == 'drop':\n",
        "                continue\n",
        "            if isinstance(cols, (list, tuple)):\n",
        "                col_names.extend(list(cols))\n",
        "            else:\n",
        "                # if col spec is slice or array, try to coerce\n",
        "                try:\n",
        "                    col_names.extend(list(cols))\n",
        "                except:\n",
        "                    pass\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if not col_names:\n",
        "        print(\"Could not recover original column names from preprocessor. Please provide a CSV row instead.\")\n",
        "        return\n",
        "\n",
        "    input_data = {}\n",
        "    for c in col_names:\n",
        "        # Provide basic guidance: if numeric, default 0; otherwise empty string\n",
        "        val = input(f\"Enter value for {c} (example/default empty): \")\n",
        "        if val.strip() == \"\":\n",
        "            input_data[c] = np.nan\n",
        "        else:\n",
        "            input_data[c] = val\n",
        "\n",
        "    # Convert to DataFrame with single row\n",
        "    single_df = pd.DataFrame([input_data], columns=col_names)\n",
        "\n",
        "    # Attempt to coerce numeric columns to numeric where appropriate (we'll detect from preprocessor)\n",
        "    # We'll try casting the numeric columns from the preprocessor\n",
        "    numeric_cols = []\n",
        "    for name, trans, cols in preprocessor.transformers_:\n",
        "        if name == 'num':\n",
        "            numeric_cols = list(cols)\n",
        "    for c in numeric_cols:\n",
        "        if c in single_df.columns:\n",
        "            single_df[c] = pd.to_numeric(single_df[c], errors='coerce')\n",
        "\n",
        "    # Fill NaNs reasonably: numeric -> 0, categorical -> 'No' or ''\n",
        "    for c in single_df.columns:\n",
        "        if single_df[c].dtype.kind in 'biufc':\n",
        "            if pd.isna(single_df.at[0, c]):\n",
        "                single_df.at[0, c] = 0\n",
        "        else:\n",
        "            if pd.isna(single_df.at[0, c]):\n",
        "                single_df.at[0, c] = ''\n",
        "\n",
        "    # Transform\n",
        "    X_single = preprocessor.transform(single_df)\n",
        "\n",
        "    # Choose model\n",
        "    choice = input(\"Which model to use for prediction? [logistic / rf] (default: rf): \").strip().lower()\n",
        "    if choice == '' or choice not in ['logistic', 'rf']:\n",
        "        choice = 'rf'\n",
        "\n",
        "    if choice == 'logistic':\n",
        "        prob = log_model.predict_proba(X_single)[0,1]\n",
        "        pred = log_model.predict(X_single)[0]\n",
        "    else:\n",
        "        prob = rf_model.predict_proba(X_single)[0,1]\n",
        "        pred = rf_model.predict(X_single)[0]\n",
        "\n",
        "    print(f\"\\nModel: {choice.upper()}  -> Predicted Churn: {'Yes' if pred==1 else 'No'}  (probability={prob:.4f})\")\n",
        "\n",
        "# --------------------- Main -----------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Telco Customer Churn Prediction Script\")\n",
        "    print(\"Dataset reference: Kaggle 'Telco Customer Churn' (blastchar). If you haven't downloaded it, place the CSV named:\\n  \", DATA_FILENAME)\n",
        "\n",
        "    # 1) Load dataset\n",
        "    df = load_dataset()\n",
        "    print(\"Dataset shape:\", df.shape)\n",
        "    print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "    # 2) Preprocess (fit)\n",
        "    X_all, y_all, preprocessor, feature_names = preprocess(df, fit_transform=True, preprocessor=None)\n",
        "    print(\"Preprocessing complete. Number of features after transform:\", X_all.shape[1])\n",
        "\n",
        "    # 3) Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=RANDOM_STATE, stratify=y_all)\n",
        "    print(\"Train/Test sizes:\", X_train.shape[0], X_test.shape[0])\n",
        "\n",
        "    # 4) Train and evaluate\n",
        "    log_model, rf_model, results = train_and_evaluate(X_train, X_test, y_train, y_test, feature_names)\n",
        "\n",
        "    # 5) Save artifacts\n",
        "    save_artifacts(preprocessor, feature_names, log_model, rf_model, results)\n",
        "\n",
        "    # 6) Interactive prediction loop (optional)\n",
        "    while True:\n",
        "        resp = input(\"\\nDo you want to make an interactive prediction now? (y/n): \").strip().lower()\n",
        "        if resp in ['y','yes']:\n",
        "            interactive_prediction(preprocessor, feature_names, log_model, rf_model)\n",
        "        else:\n",
        "            print(\"Exiting interactive mode.\")\n",
        "            break\n",
        "\n",
        "    print(\"Script finished. Artifacts saved in:\", ARTIFACTS_DIR)\n"
      ]
    }
  ]
}